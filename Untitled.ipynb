{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0313d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dual_ArbNet.model.resblock import ResBlock\n",
    "\n",
    "\n",
    "class SineAct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "    \n",
    "    \n",
    "class ImplicitDecoder(nn.Module):\n",
    "    def __init__(self, in_channels=64, hidden_dims=[64, 64, 64, 64, 64]):\n",
    "        super().__init__()\n",
    "\n",
    "        last_dim_K = in_channels * 9 + in_channels * 9\n",
    "        \n",
    "        last_dim_Q = 4\n",
    "\n",
    "        self.K = nn.ModuleList()\n",
    "        self.Q = nn.ModuleList()\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.K.append(nn.Sequential(nn.Conv2d(last_dim_K, hidden_dim*2, 1),\n",
    "                                        nn.ReLU(),\n",
    "                                        ResBlock(channels = hidden_dim*2, nConvLayers = 4)\n",
    "                                        ))    \n",
    "            self.Q.append(nn.Sequential(nn.Conv2d(last_dim_Q, hidden_dim, 1),\n",
    "                                        SineAct()))\n",
    "            last_dim_K = hidden_dim*2\n",
    "            last_dim_Q = hidden_dim\n",
    "        self.last_layer = nn.Conv2d(hidden_dims[-1], 2, 1)\n",
    "        self.ref_branch = nn.Sequential(nn.Conv2d(in_channels * 9, hidden_dims[-2], 1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-2],hidden_dims[-1], 1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1],2, 1),\n",
    "                            nn.ReLU())\n",
    "        self.in_branch = nn.Sequential(nn.Conv2d(in_channels * 9, hidden_dims[-2], 1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-2],hidden_dims[-1], 1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1],2, 1),\n",
    "                            nn.ReLU())\n",
    "        \n",
    "    def _make_pos_encoding(self, x, size): \n",
    "        B, C, H, W = x.shape\n",
    "        H_up, W_up = size\n",
    "       \n",
    "        h_idx = -1 + 1/H + 2/H * torch.arange(H, device=x.device).float()\n",
    "        w_idx = -1 + 1/W + 2/W * torch.arange(W, device=x.device).float()\n",
    "        in_grid = torch.stack(torch.meshgrid(h_idx, w_idx), dim=0)\n",
    "\n",
    "        h_idx_up = -1 + 1/H_up + 2/H_up * torch.arange(H_up, device=x.device).float()\n",
    "        w_idx_up = -1 + 1/W_up + 2/W_up * torch.arange(W_up, device=x.device).float()\n",
    "        up_grid = torch.stack(torch.meshgrid(h_idx_up, w_idx_up), dim=0)\n",
    "        \n",
    "        rel_grid = (up_grid - F.interpolate(in_grid.unsqueeze(0), size=(H_up, W_up), mode='nearest-exact'))\n",
    "        rel_grid[:,0,:,:] *= H\n",
    "        rel_grid[:,1,:,:] *= W\n",
    "\n",
    "        return rel_grid.contiguous().detach()\n",
    "\n",
    "    def step(self, x, ref, syn_inp):\n",
    "        q = syn_inp\n",
    "        q_ref =syn_inp\n",
    "        k = x\n",
    "        k_ref = ref\n",
    "        kk = torch.cat([k,k_ref],dim=1)\n",
    "        for i in range(len(self.K)):\n",
    "            kk = self.K[i](kk)\n",
    "            dim = kk.shape[1]//2\n",
    "            q = kk[:,:dim]*self.Q[i](q)\n",
    "            q_ref = kk[:,dim:]*self.Q[i](q_ref)\n",
    "        q = self.last_layer(q)\n",
    "        q_ref = self.last_layer(q_ref)\n",
    "        return q + self.in_branch(x) ,q_ref + self.ref_branch(ref)\n",
    "\n",
    "    def batched_step(self, x, syn_inp, bsize):\n",
    "        with torch.no_grad():\n",
    "            h, w = syn_inp.shape[-2:]\n",
    "            ql = 0\n",
    "            preds = []\n",
    "            while ql < w:\n",
    "                qr = min(ql + bsize//h, w)\n",
    "                pred = self.step(x[:, :, :, ql: qr], syn_inp[:, :, :, ql: qr])\n",
    "                preds.append(pred)\n",
    "                ql = qr\n",
    "            pred = torch.cat(preds, dim=-1)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def forward(self, x, ref, size, bsize=None):\n",
    "        B, C, H_in, W_in = x.shape\n",
    "        Bref, Cref, H_in_ref, W_in_ref = ref.shape\n",
    "        rel_coord = (self._make_pos_encoding(x, size).expand(B, -1, *size))\n",
    "        ratio = (x.new_tensor([math.sqrt((H_in*W_in)/(size[0]*size[1]))]).view(1, -1, 1, 1).expand(B, -1, *size))\n",
    "        ratio_ref = (ref.new_tensor([math.sqrt((H_in_ref*W_in_ref)/(size[0]*size[1]))]).view(1, -1, 1, 1).expand(Bref, -1, *size))\n",
    "        syn_inp = torch.cat([rel_coord, ratio, ratio_ref], dim=1)\n",
    "        x = F.interpolate(F.unfold(x, 3, padding=1).view(B, C*9, H_in, W_in), size=syn_inp.shape[-2:], mode='bilinear')\n",
    "        ref = F.interpolate(F.unfold(ref, 3, padding=1).view(B, C*9, H_in_ref, W_in_ref), size=syn_inp.shape[-2:], mode='bilinear')\n",
    "        if bsize is None: \n",
    "            pred = self.step(x, ref, syn_inp)\n",
    "        else:\n",
    "            pred = self.batched_step(x, syn_inp, bsize)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b01fa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_norm_2d(x, kernel_size=3):\n",
    "    mean = F.avg_pool2d(x, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "    mean_sq = F.avg_pool2d(x**2, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "    var = mean_sq - mean**2\n",
    "    return (x-mean)/(var + 1e-6)\n",
    "\n",
    "class DUALRef(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.encoder = make_rdn()\n",
    "        self.decoder = ImplicitDecoder()\n",
    "        self.mixer = nn.Conv2d(64*2, 64, 1, padding=0, stride=1)\n",
    "    \n",
    "    def set_scale(self, scale, scale2):\n",
    "        self.scale = scale\n",
    "        self.scale2 = scale2\n",
    "\n",
    "    def forward(self, inp, bsize = None):\n",
    "        if len(inp)==5:\n",
    "            epoch = inp[4]\n",
    "        else:\n",
    "            epoch = None\n",
    "        ref_type = inp[3]\n",
    "        if ref_type == None:\n",
    "            ref_type = random.randint(1,2) \n",
    "            if epoch is not None and epoch < 10:\n",
    "                ref_type = 1\n",
    "        ref = inp[ref_type] \n",
    "        inp = inp[0]\n",
    "\n",
    "        B,C,H,W = inp.shape\n",
    "        B,C,H_ref,W_ref = ref.shape\n",
    "        H_hr = round(H*self.scale)\n",
    "        W_hr = round(W*self.scale2)\n",
    "        feat = self.encoder((inp-0.5)/0.5)\n",
    "        with torch.no_grad():\n",
    "            ref = self.encoder((ref-0.5)/0.5)\n",
    "        ref.requires_grad = True\n",
    "        size = [H_hr, W_hr]\n",
    "        pred,pred_ref = self.decoder(feat, ref, size, bsize)\n",
    "\n",
    "        return pred*0.5+0.5, pred_ref*0.5+0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72f40c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dual_ArbNet.model.rdn import make_rdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "352956cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = make_rdn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ImplicitDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cda515d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_hooks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/DMRI_SR/Dual_ArbNet/model/rdn.py:77\u001b[0m, in \u001b[0;36mRDN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 77\u001b[0m     f__1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSFENet1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     x  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSFENet2(f__1)\n\u001b[1;32m     80\u001b[0m     RDBs_out \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_hooks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "feat = encoder((x[0]-0.5)/0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28b62539",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred,pred_ref \u001b[38;5;241m=\u001b[39m decoder(\u001b[43mfeat\u001b[49m, ref, size, bsize)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat' is not defined"
     ]
    }
   ],
   "source": [
    "pred,pred_ref = decoder(feat, ref, size, bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "443d00ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][:,:,0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d4e58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[0][:,:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd7cb638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][:,:,0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91450e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RDN(\n",
       "  (SFENet1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (SFENet2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (RDBs): ModuleList(\n",
       "    (0-4): 5 x RDB(\n",
       "      (convs): Sequential(\n",
       "        (0): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (2): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (3): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (4): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (5): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (6): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (7): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (8): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (9): RDB_Conv(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(640, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (LFF): Conv2d(704, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (GFF): Sequential(\n",
       "    (0): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27ee3d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'apath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapath\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'apath'"
     ]
    }
   ],
   "source": [
    "args.apath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8217a103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.ref_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c4d6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dual_ArbNet import model as dualarb_model\n",
    "from Dual_ArbNet.option import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee42e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model...\n"
     ]
    }
   ],
   "source": [
    "model = dualarb_model.Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92af7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): DataParallel(\n",
       "    (module): DUALRef(\n",
       "      (encoder): RDN(\n",
       "        (SFENet1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (SFENet2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (RDBs): ModuleList(\n",
       "          (0-4): 5 x RDB(\n",
       "            (convs): Sequential(\n",
       "              (0): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (1): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (2): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (3): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (4): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (5): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (6): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (7): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (8): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (9): RDB_Conv(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(640, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (LFF): Conv2d(704, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (GFF): Sequential(\n",
       "          (0): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (decoder): ImplicitDecoder(\n",
       "        (K): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "            (2): ResBlock(\n",
       "              (convs): Sequential(\n",
       "                (0): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (1): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (2): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (3): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (CBAM): CBAM(\n",
       "                (ca): ChannelAttention(\n",
       "                  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu1): ReLU()\n",
       "                  (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (sa): SpatialAttention(\n",
       "                  (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-4): 4 x Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): ReLU()\n",
       "            (2): ResBlock(\n",
       "              (convs): Sequential(\n",
       "                (0): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (1): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (2): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (3): Res_Conv(\n",
       "                  (conv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (1): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (CBAM): CBAM(\n",
       "                (ca): ChannelAttention(\n",
       "                  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu1): ReLU()\n",
       "                  (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (sa): SpatialAttention(\n",
       "                  (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Q): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): SineAct()\n",
       "          )\n",
       "          (1-4): 4 x Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): SineAct()\n",
       "          )\n",
       "        )\n",
       "        (last_layer): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (ref_branch): Sequential(\n",
       "          (0): Conv2d(576, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): ReLU()\n",
       "        )\n",
       "        (in_branch): Sequential(\n",
       "          (0): Conv2d(576, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (mixer): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0934c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import cucim\n",
    "import data.utils_metrics as utils_met\n",
    "import cucim.skimage.metrics as met\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac067f4",
   "metadata": {},
   "source": [
    "7T scan resolution : 173, 207, 173\n",
    "\n",
    "3T scan resolution : 145, 174, 143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3cfb2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=4, block_size=(64, 64, 1), crop_depth=30, debug=False, dir='/storage', preload=True, sort=True, test_size=(173, 207, 1), typ='upsampled')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"IMDN\")\n",
    "parser.add_argument(\"--block_size\", type=tuple, default=(64,64,1),\n",
    "                    help=\"Block Size\")\n",
    "parser.add_argument(\"--test_size\", type=tuple, default=(173,207,1),\n",
    "                    help=\"test_size\")\n",
    "parser.add_argument(\"--crop_depth\", type=int, default=30,\n",
    "                    help=\"crop across z-axis\")\n",
    "parser.add_argument(\"--dir\", type=str,\n",
    "                    help=\"dataset_directory\")\n",
    "parser.add_argument(\"--batch_size\", type=int,\n",
    "                    help=\"dataset_directory\")\n",
    "parser.add_argument(\"--sort\", type=bool,\n",
    "                    help=\"dataset_directory\")\n",
    "parser.add_argument(\"--debug\", type=bool,\n",
    "                    help=\"dataset_directory\")\n",
    "parser.add_argument(\"--preload\", type=bool,\n",
    "                    help=\"dataset_directory\")\n",
    "args = list(parser.parse_known_args())[0]\n",
    "args.preload = True\n",
    "args.debug = False\n",
    "args.dir = \"/storage\"\n",
    "args.batch_size = 4\n",
    "args.sort = True\n",
    "args.typ = 'upsampled'\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f20f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of common Subjects  171\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import data.HCP_dataset_h5 as HCP_dataset\n",
    "\n",
    "ids = utils.get_ids()\n",
    "ids.sort()\n",
    "ids = ids[:5]\n",
    "dataset_hcp = HCP_dataset\n",
    "dataset_hcp.load_data(args.dir,ids)\n",
    "training_dataset = dataset_hcp.hcp_data(args,ids)\n",
    "testing_dataset = dataset_hcp.hcp_data_test(args,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7816756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 1, 8) (64, 64, 1) (64, 64, 1) (64, 64, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "x = training_dataset[0]\n",
    "print(x[0].shape,x[1].shape,x[2].shape,x[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "494e84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_model().set_scale(1.1,1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71ba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
